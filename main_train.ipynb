{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, json, os, re, sys, time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorboardX\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fields import Field, Parms, Semantic, Vocab, _make_vocab\n",
    "from utils import *\n",
    "\n",
    "from nlp_db import nlp_db\n",
    "\n",
    "from model_class import NLU_Classify\n",
    "\n",
    "semantic = Semantic()\n",
    "args = Parms()\n",
    "vocab = Vocab(semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d5e3390393bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanul_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./manuaLog_lstm1.log'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "args.manul_log = './manuaLog_lstm1.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file, thresh=np.infty, k=None, func=None):\n",
    "\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        rzlt = []\n",
    "        cnt = 0\n",
    "        for l in f.readlines():\n",
    "\n",
    "            if k != None and func != None:\n",
    "                rzlt.append(func(json.loads(l)[k]))\n",
    "\n",
    "            elif k != None:\n",
    "                rzlt.append(json.loads(l)[k])\n",
    "\n",
    "            else:\n",
    "                rzlt.append(json.loads(l))\n",
    "\n",
    "            if cnt > thresh:\n",
    "                break\n",
    "\n",
    "    return rzlt\n",
    "\n",
    "\n",
    "def json_iter(file, batch_size=1000, k=None, func=None):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        rzlt = []\n",
    "        for l in f.readlines():\n",
    "            if k != None and func != None:\n",
    "                rzlt.append(func(json.loads(l)[k]))\n",
    "\n",
    "            elif k != None:\n",
    "                rzlt.append(json.loads(l)[k])\n",
    "\n",
    "            else:\n",
    "                rzlt.append(json.loads(l))\n",
    "\n",
    "            if len(rzlt) == batch_size:\n",
    "\n",
    "                yield rzlt\n",
    "                rzlt = []\n",
    "                \n",
    "def func_pad(sent, max_sent_len):\n",
    "    return [vocab.__getitem__(token) for token in jieba.cut(sent)\n",
    "            ] + [0] * (max_sent_len - len(list(jieba.cut(sent)))) , len(list(jieba.cut(sent)))\n",
    "\n",
    "\n",
    "\n",
    "def acc(y_hat, y_label):\n",
    "    correct = (torch.argmax(y_hat, dim = 1) == y_label).float()\n",
    "    acc_rate = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc_rate\n",
    "\n",
    "def dump_log():\n",
    "    with open('./manuLog_lstm1.log', 'a') as fp:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"epoch\": last_epoch,\n",
    "                \"loss\": last_loss.data.item(),\n",
    "                \"train_avg_acc\": last_avgac.data.item(),\n",
    "                \"dev_avg_acc\": dev_acc.data.item()\n",
    "            }, fp)\n",
    "        fp.write('\\n')\n",
    "\n",
    "    with open('./manuLog_lstm1.log', 'r') as fp:\n",
    "        last_line = fp.readlines()[-10:]\n",
    "    \n",
    "    with open('./manuLog_lstm1.log', 'w') as fp:\n",
    "        fp.write(last_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './manuLog_lstm1.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4f55bcbcd810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./manuLog_lstm1.log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlast_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./manuLog_lstm1.log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './manuLog_lstm1.log'"
     ]
    }
   ],
   "source": [
    "with open('./manuLog_lstm1.log', 'r') as fp:\n",
    "    last_line = fp.readlines()[-10:]\n",
    "\n",
    "with open('./manuLog_lstm1.log', 'w') as fp:\n",
    "    fp.write(last_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/Studio/nlp_db/tnews_public/labels.json', '/home/ubuntu/Studio/nlp_db/tnews_public/test.json', '/home/ubuntu/Studio/nlp_db/tnews_public/train.json', '/home/ubuntu/Studio/nlp_db/tnews_public/vocab.txt', '/home/ubuntu/Studio/nlp_db/tnews_public/.ipynb_checkpoints', '/home/ubuntu/Studio/nlp_db/tnews_public/dev.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/Studio/nlp_db/tnews_public/test.json',\n",
       " '/home/ubuntu/Studio/nlp_db/tnews_public/train.json',\n",
       " '/home/ubuntu/Studio/nlp_db/tnews_public/vocab.txt',\n",
       " '/home/ubuntu/Studio/nlp_db/tnews_public/dev.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_path = \"../nlp_db/tnews_public\"\n",
    "cfiles = [\n",
    "    os.path.join(os.path.abspath(rel_path),\n",
    "                 os.listdir(rel_path)[i])\n",
    "    for i in range(len(os.listdir(rel_path)))\n",
    "]\n",
    "print(cfiles)\n",
    "testFile = cfiles[1]\n",
    "trainFile = cfiles[2]\n",
    "vocabFile = cfiles[3]\n",
    "devFile = cfiles[-1]\n",
    "testFile, trainFile, vocabFile, devFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.593 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16718, 1061, 0, 3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.vocab_path = vocabFile\n",
    "\n",
    "os.remove(args.vocab_path)\n",
    "if not os.path.isfile(args.vocab_path):\n",
    "    _make_vocab(json_file=trainFile,\n",
    "                vocab_path=args.vocab_path,\n",
    "                thres=2,\n",
    "                level='word')\n",
    "\n",
    "try:\n",
    "    vocab.load(args.vocab_path)\n",
    "except:\n",
    "    print(\"Vocab not loaded\")\n",
    "vocab.size, vocab.__getitem__('ÂêÉ'), vocab.__getitem__(\n",
    "    '<pad>'), vocab.__getitem__('<unk>'), vocab.__getitem__('<sos>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Process => Model Parms Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 5000,\n",
       " 'class_num': 15,\n",
       " 'exts': ['.en.atok', '.de.atok'],\n",
       " 'lstm_hid': 64,\n",
       " 'lstm_step_num': 2,\n",
       " 'manul_log': './manuaLog_lstm1.log',\n",
       " 'max_dec_num': 50,\n",
       " 'max_enc_num': 50,\n",
       " 'max_sent_len': 81,\n",
       " 'modes': ['train', 'val', 'test2016'],\n",
       " 'n_sent': 5,\n",
       " 'ndev': 1,\n",
       " 'path': './data/multi30k/',\n",
       " 'vocab_path': '/home/ubuntu/Studio/nlp_db/tnews_public/vocab.txt'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_len = max([\n",
    "    len(line) for line in read_json(\n",
    "        trainFile, 60000, k='sentence', func=lambda x: list(jieba.cut(x)))\n",
    "])\n",
    "args.max_sent_len = max_sent_len\n",
    "\n",
    "labels = read_json(cfiles[0], 100, k='label')\n",
    "label_rdict = {l:i for i,l in enumerate(labels)}\n",
    "label_dict = {i:l for i,l in enumerate(labels)}\n",
    "\n",
    "\n",
    "args.class_num = len(labels)\n",
    "\n",
    "args.max_sent_len = max_sent_len\n",
    "args.lstm_step_num = 2\n",
    "args.lstm_hid = 64\n",
    "\n",
    "args.batch_size = 5000\n",
    "dirrm(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Data Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLU_Classify(nn.Module):\n",
    "    def __init__(self, class_num, vocab, args):\n",
    "        super(NLU_Classify, self).__init__()\n",
    "        self.type = 'classifier'\n",
    "        self.batch_size = args.batch_size\n",
    "        self.serial_len = 2\n",
    "        self.emb = nn.Embedding(vocab.size, embedding_dim=128)\n",
    "        self.lstm = nn.LSTM(128,\n",
    "                            args.lstm_hid,\n",
    "                            args.lstm_step_num,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(64, class_num)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, sent_lengths):\n",
    "        # ? not sure serial_len , batch_size is 100% right\n",
    "        embedded_x = self.emb(x)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_x,\n",
    "                                                    sent_lengths,\n",
    "                                                    enforce_sorted=False,\n",
    "                                                    batch_first=True)\n",
    "        h0 = torch.randn(self.serial_len, self.batch_size, args.lstm_hid, device = device)\n",
    "        c0 = torch.randn(self.serial_len, self.batch_size, args.lstm_hid, device = device)\n",
    "        x, (hidden, cn) = self.lstm(packed_embedded, (h0, c0))\n",
    "        hidden = hidden[-1,:,:]\n",
    "        output = self.fc(hidden)\n",
    "        output = self.softmax(output)\n",
    "        result = output\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model, loss, optimizer - Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NLU_Classify(class_num=args.class_num, vocab=vocab, args = args)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 5000,\n",
       " 'class_num': 15,\n",
       " 'dev_max_sent_len': 72,\n",
       " 'exts': ['.en.atok', '.de.atok'],\n",
       " 'lstm_hid': 64,\n",
       " 'lstm_step_num': 2,\n",
       " 'max_dec_num': 50,\n",
       " 'max_enc_num': 50,\n",
       " 'max_sent_len': 81,\n",
       " 'modes': ['train', 'val', 'test2016'],\n",
       " 'n_sent': 5,\n",
       " 'ndev': 1,\n",
       " 'path': './data/multi30k/',\n",
       " 'vocab_path': '/home/ubuntu/Studio/nlp_db/tnews_public/vocab.txt'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_max_sent_len = max([\n",
    "    len(line) for line in read_json(\n",
    "        devFile, k='sentence', func=lambda x: list(jieba.cut(x)))\n",
    "])\n",
    "args.dev_max_sent_len = dev_max_sent_len\n",
    "\n",
    "dirrm(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation\n",
    "- Start from last checkpoint\n",
    "- Matrix Capture\n",
    "- Stop Rules ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_epoch():\n",
    "    with open(args.manual_log, 'r') as f:\n",
    "        l = f.readlines()[-1]\n",
    "        last_epoch = json.loads(l.strip())['epoch']\n",
    "        \n",
    "    return last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restart_iter(batch_size, datafile):\n",
    "    x_iter = json_iter(\n",
    "        file=datafile,\n",
    "        batch_size=batch_size,\n",
    "        k='sentence',\n",
    "        func=lambda sent: func_pad(sent, max_sent_len=args.max_sent_len))\n",
    "\n",
    "    y_iter = json_iter(file=datafile,\n",
    "                       batch_size=batch_size,\n",
    "                       k='label',\n",
    "                       func=lambda x: label_rdict[x])\n",
    "\n",
    "    return x_iter, y_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for Eval\n",
    "eval_x, eval_sent_lengths = list(\n",
    "    zip(*read_json(\n",
    "        devFile,\n",
    "        k='sentence',\n",
    "        thresh=np.infty,\n",
    "        func=lambda sent: func_pad(sent, max_sent_len=args.dev_max_sent_len))))\n",
    "\n",
    "eval_y = read_json(file=devFile, k='label', func=lambda x: label_rdict[x])\n",
    "\n",
    "eval_sent_lengths = torch.tensor(eval_sent_lengths)\n",
    "eval_x = torch.tensor(np.array([np.array(line) for line in eval_x]))\n",
    "eval_y = torch.tensor(eval_y)\n",
    "\n",
    "eval_x = eval_x.to(device)\n",
    "eval_sent_lengths = eval_sent_lengths.to(device)\n",
    "eval_y = eval_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "\n",
    "if not os.path.isdir('./model_stores'):\n",
    "    os.mkdir('./model_stores')\n",
    "\n",
    "args.model_path = './model_stores/model1.pth'\n",
    "first_train = True\n",
    "\n",
    "# Load:\n",
    "if os.path.isfile(args.model_path) and first_train == False:\n",
    "    model.load_state_dict(torch.load(args.model_path))\n",
    "    model.train()  # set model to train mode\n",
    "\n",
    "\n",
    "if first_train:\n",
    "    last_epoch = 1\n",
    "    try:\n",
    "        shutil.rmtree(os.path.abspath('./runs/')) \n",
    "        os.remove(os.path.abspath(args.manul_log))\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    last_epoch = get_last_epoch()\n",
    "\n",
    "    \n",
    "\n",
    "writer_train = tensorboardX.SummaryWriter('runs/train_0')\n",
    "writer_test = tensorboardX.SummaryWriter('runs/test_0')\n",
    "writer = tensorboardX.SummaryWriter('runs/net_0')\n",
    "writer.add_graph(model, eval_x)\n",
    "\n",
    "\n",
    "epoch = last_epoch\n",
    "if not \"acc_rates\" in locals():\n",
    "    acc_rates = [0] * 10\n",
    "\n",
    "while True:\n",
    "    # while np.array(acc_rates).sum() / len(acc_rates) < 0.8:\n",
    "    epoch += 1\n",
    "    x_iter, y_iter = restart_iter(args.batch_size, trainFile)\n",
    "\n",
    "    ep_cnt = 0\n",
    "    acc_loss = []\n",
    "    acc_rates = []\n",
    "    for batch_x, batch_y in zip(x_iter, y_iter):\n",
    "        model.train()\n",
    "        batch_x, sent_lengths = list(zip(*batch_x))\n",
    "\n",
    "        batch_x = torch.tensor(np.array([np.array(line) for line in batch_x]))\n",
    "        sent_lengths = torch.tensor(sent_lengths)\n",
    "        batch_y = torch.tensor(batch_y)\n",
    "\n",
    "        batch_x = batch_x.to(device)\n",
    "        sent_lengths = sent_lengths.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(batch_x, sent_lengths)\n",
    "        loss = loss_func(y_hat, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        # loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_rate = acc(y_hat, batch_y)\n",
    "\n",
    "        ep_cnt += 1\n",
    "        acc_loss.append(loss)\n",
    "        acc_rates.append(acc_rate)\n",
    "        if ep_cnt % 10 == 0:\n",
    "            # get metrics\n",
    "            idx = epoch + 0.32 * (ep_cnt % 20)\n",
    "            last_loss, last_avgac = np.array(acc_loss).sum() / len(\n",
    "                acc_loss), np.array(acc_rates).sum() / len(acc_rates)\n",
    "\n",
    "            print(epoch, \"loss: \", last_loss.data, \"Acc: \", last_avgac)\n",
    "\n",
    "            writer_train.add_scalar('loss', last_loss, idx)\n",
    "            writer_train.add_scalar('train_avgAcc:', last_avgac, idx)\n",
    "\n",
    "            acc_loss = []\n",
    "            acc_rates = []\n",
    "\n",
    "            # Save Model Parameters:\n",
    "            torch.save(model.state_dict(), f=args.model_path)\n",
    "\n",
    "            # Eval\n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(eval_x, eval_sent_lengths)\n",
    "            dev_acc = acc(yhat, eval_y)\n",
    "\n",
    "            print(epoch, \"dev_acc: \", dev_acc)\n",
    "\n",
    "            writer_test.add_scalar('dev_avgAcc', dev_acc.data.item(), idx)\n",
    "\n",
    "            last_epoch = epoch\n",
    "\n",
    "            dump_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Trained Parameters\n",
    "# model.state_dict()    # is a ordered dict\n",
    "# {k: model.state_dict()[k].shape for k in model.state_dict()}"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
