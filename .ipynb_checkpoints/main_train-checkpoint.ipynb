{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, json, os, re, sys, time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fields import Field, Parms, Semantic, Vocab, _make_vocab\n",
    "from utils import *\n",
    "\n",
    "from nlp_db import nlp_db\n",
    "\n",
    "from model_class import NLU_Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file, thresh=20, k=None, func=None):\n",
    "\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        rzlt = []\n",
    "        cnt = 0\n",
    "        for l in f.readlines():\n",
    "\n",
    "            if k != None and func != None:\n",
    "                rzlt.append(func(json.loads(l)[k]))\n",
    "\n",
    "            elif k != None:\n",
    "                rzlt.append(json.loads(l)[k])\n",
    "\n",
    "            else:\n",
    "                rzlt.append(json.loads(l))\n",
    "\n",
    "            if cnt > thresh:\n",
    "                break\n",
    "\n",
    "    return rzlt\n",
    "\n",
    "\n",
    "def json_iter(file, batch_size=1000, k=None, func=None):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        rzlt = []\n",
    "        for l in f.readlines():\n",
    "            if k != None and func != None:\n",
    "                rzlt.append(func(json.loads(l)[k]))\n",
    "\n",
    "            elif k != None:\n",
    "                rzlt.append(json.loads(l)[k])\n",
    "\n",
    "            else:\n",
    "                rzlt.append(json.loads(l))\n",
    "\n",
    "            if len(rzlt) == batch_size:\n",
    "\n",
    "                yield rzlt\n",
    "                rzlt = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2124000\r\n",
      "-rw-r--r--  1 root  staff   21695199 Jun  7 13:48 xiaohuangji.tsv\r\n",
      "-rw-r--r--  1 root  staff  465372773 Jun  7 13:48 weibo.tsv\r\n",
      "-rw-r--r--  1 root  staff  298597018 Jun  7 13:46 tieba.tsv\r\n",
      "-rw-r--r--  1 root  staff  151548740 Jun  7 13:46 subtitle.tsv\r\n",
      "-rw-r--r--  1 root  staff    5594328 Jun  7 13:45 qingyun.tsv\r\n",
      "-rw-r--r--  1 root  staff   18202714 Jun  7 13:45 ptt.tsv\r\n",
      "-rw-r--r--  1 root  staff      34249 Jun  7 13:44 chatterbot.tsv\r\n",
      "-rw-r--r--  1 root  staff   85680288 Jun  7 13:44 douban_single_turn.tsv\r\n"
     ]
    }
   ],
   "source": [
    "%ls -lct ~/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Files():\n",
    "    def __init__(self,):\n",
    "#         self.time = datetime.now()\n",
    "        pass\n",
    "\n",
    "files = Files()\n",
    "\n",
    "path = os.path.abspath(\"../dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus\")\n",
    "file_nms = os.listdir(path)\n",
    "\n",
    "for i in range(len(file_nms)):\n",
    "    setattr(files, file_nms[i].split('.')[0], os.path.join(path, file_nms[i]))\n",
    "    \n",
    "# dirrs(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for name in file_nms:\n",
    "    file = os.path.join(path, name)\n",
    "    with open(file, 'r') as f:\n",
    "        num = len(f.readlines())\n",
    "        print(name,\":\", \"{:,d}\".format(num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "with open(files.ptt, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)\n",
    "        cnt += 1\n",
    "        if cnt > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = \"../nlp_db/tnews_public\"\n",
    "cfiles = [\n",
    "    os.path.join(os.path.abspath(rel_path),\n",
    "                 os.listdir(rel_path)[i])\n",
    "    for i in range(len(os.listdir(rel_path)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cfiles[4]),list(read_json(cfiles[2],100,'sentence', lambda x: list(jieba.cut(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(cfiles[4]),(read_json(cfiles[0],100,k = 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = Semantic()\n",
    "args = Parms()\n",
    "args.path = \"../nlp_db/tnews_public\"\n",
    "args.vocab_path = \"../nlp_db/tnews_public/vocab.txt\"\n",
    "vocab = Vocab(semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(args.vocab_path):\n",
    "\n",
    "    _make_vocab(json_file = cfiles[3], vocab_path = args.vocab_path, thres=2, level = 'word')\n",
    "    # or just make new vocab\n",
    "    # char level ?\n",
    "    # or chinese word level | with jieba\n",
    "\n",
    "try:\n",
    "    vocab.load(args.vocab_path)\n",
    "except:\n",
    "    print(\"Vocab not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16718, 1061)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.size, vocab.__getitem__('åƒ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Process => Model Parms Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.634 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "max_sent_len = max([\n",
    "    len(line) for line in read_json(\n",
    "        cfiles[4], 60000, k='sentence', func=lambda x: list(jieba.cut(x)))\n",
    "])\n",
    "args.max_sent_len = max_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = read_json(cfiles[0], 100, k='label')\n",
    "label_rdict = {l:i for i,l in enumerate(set([lb[0] for lb in labels]))}\n",
    "label_dict = {i:l for i,l in enumerate(set([lb[0] for lb in labels]))}\n",
    "\n",
    "\n",
    "args.class_num = len(labels)\n",
    "\n",
    "args.max_sent_len = max_sent_len\n",
    "args.lstm_step_num = 2\n",
    "args.lstm_hid = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1000,\n",
       " 'class_num': 15,\n",
       " 'exts': ['.en.atok', '.de.atok'],\n",
       " 'lstm_hid': 64,\n",
       " 'lstm_step_num': 2,\n",
       " 'max_dec_num': 50,\n",
       " 'max_enc_num': 50,\n",
       " 'max_sent_len': 81,\n",
       " 'modes': ['train', 'val', 'test2016'],\n",
       " 'n_sent': 5,\n",
       " 'ndev': 1,\n",
       " 'path': '../nlp_db/tnews_public',\n",
       " 'vocab_path': '../nlp_db/tnews_public/vocab.txt'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size = 1000\n",
    "dirrm(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_iter = json_iter(\n",
    "#     file=cfiles[4],\n",
    "#     batch_size=2,\n",
    "#     k='sentence',\n",
    "#     func=lambda sent: [vocab.__getitem__(token)\n",
    "#                        for token in jieba.cut(sent)] + [0] *\n",
    "#     (max_sent_len - len(list(jieba.cut(sent)))))\n",
    "\n",
    "# y_iter = json_iter(file=cfiles[4],\n",
    "#                    batch_size=2,\n",
    "#                    k='label',\n",
    "#                    func=lambda x: label_rdict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for batch_x, batch_y in zip(x_iter, y_iter):\n",
    "#     cnt += 1\n",
    "#     print(\"x:\",np.array(batch_x))\n",
    "#     print(\"y:\",np.array(batch_y))\n",
    "    \n",
    "#     if cnt > 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward, Loss & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLU_Classify(nn.Module):\n",
    "    def __init__(self, class_num, vocab):\n",
    "        super(NLU_Classify, self).__init__()\n",
    "        self.type = 'classifier'\n",
    "        self.batch_size = 1000\n",
    "        self.serial_len = 2\n",
    "        self.emb = nn.Embedding(vocab.size, embedding_dim=128)\n",
    "        self.lstm = nn.LSTM(128,\n",
    "                            args.lstm_hid,\n",
    "                            args.lstm_step_num,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(64, class_num)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ? not sure serial_len , batch_size is 100% right\n",
    "        x = self.emb(x)\n",
    "        h0 = torch.randn(self.serial_len, self.batch_size, args.lstm_hid)\n",
    "        c0 = torch.randn(self.serial_len, self.batch_size, args.lstm_hid)\n",
    "        x, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        x = self.fc(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.softmax(x)\n",
    "        result = x\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NLU_Classify(class_num=args.class_num, vocab=vocab)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "x_iter = json_iter(\n",
    "    file=cfiles[4],\n",
    "    batch_size=args.batch_size,\n",
    "    k='sentence',\n",
    "    func=lambda sent: [vocab.__getitem__(token)\n",
    "                       for token in jieba.cut(sent)] + [0] *\n",
    "    (max_sent_len - len(list(jieba.cut(sent)))))\n",
    "\n",
    "y_iter = json_iter(file=cfiles[4],\n",
    "                   batch_size=args.batch_size,\n",
    "                   k='label',\n",
    "                   func=lambda x: label_rdict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_iter, y_iter\n",
    "for batch_x, batch_y in zip(x_iter, y_iter):\n",
    "    batch_x = torch.tensor(batch_x)\n",
    "    batch_y = torch.tensor(batch_y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(batch_x)\n",
    "    loss = loss_func(y_hat, batch_y);print(loss)\n",
    "#     loss.backward(retain_graph=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def predict_to_sent(result):\n",
    "#     return [''.join([vocab.vocab_rdict[tkid.tolist()] for tkid in line]) for line in result ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
