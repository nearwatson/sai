{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import jieba, json, math, os, re, sys, shutil, time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorboardX\n",
    "\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from fields import Field, Parms, Semantic, Vocab, _make_vocab\n",
    "from utils import *\n",
    "\n",
    "from nlp_db import nlp_db\n",
    "\n",
    "from model_class import NLU_Classify\n",
    "\n",
    "semantic = Semantic()\n",
    "args = Parms()\n",
    "vocab = Vocab(semantic)\n",
    "\n",
    "args.manual_log = './manualog_transfromer1.log'\n",
    "args.model_path = './model_stores/transformer_wiki1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialog Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pathFiles(rel_path):\n",
    "    return [\n",
    "        os.path.join(os.path.abspath(rel_path),\n",
    "                     os.listdir(rel_path)[i])\n",
    "        for i in range(len(os.listdir(rel_path)))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/chatterbot.tsv\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/xiaohuangji.tsv\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/weibo.tsv\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/douban_single_turn.tsv\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/qingyun.tsv\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/vocab.txt\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/tieba.tsv\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/subtitle.tsv\n",
      "/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/ptt.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/xiaohuangji.tsv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train, test file names\n",
    "rel_path = \"../dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/\"\n",
    "cfiles = pathFiles(rel_path)\n",
    "[print(file) for file in cfiles]\n",
    "\n",
    "botTrainFile = cfiles[1]\n",
    "botTrainFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vocab Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args.vocab_path = os.path.abspath(os.path.join(rel_path, 'vocab.txt'))\n",
    "\n",
    "def _make_chatbot_vocab(file, vocab_path, thres = 2):\n",
    "    word_dict = {}\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        cnt = 0\n",
    "        for l in f.readlines():\n",
    "            for token in list(jieba.cut(l.strip().replace('\\t',\"\"))):\n",
    "                if token not in word_dict:\n",
    "                    word_dict[token] = 0\n",
    "                else:\n",
    "                    word_dict[token] += 1\n",
    "\n",
    "    if not os.path.isfile(vocab_path):\n",
    "        open(vocab_path,'a').close()\n",
    "\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        for k, v in word_dict.items():\n",
    "            if v > thres:\n",
    "                print(k, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32970, 527, 263, 266, 264)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.remove(args.vocab_path)\n",
    "if not os.path.isfile(args.vocab_path):\n",
    "    _make_chatbot_vocab(botTrainFile, args.vocab_path)\n",
    "\n",
    "try:\n",
    "    vocab.load(args.vocab_path)\n",
    "except:\n",
    "    print(\"Vocab not loaded\")\n",
    "vocab.size, vocab.__getitem__('ÂêÉ'), vocab.__getitem__(\n",
    "    '<pad>'), vocab.__getitem__('<unk>'), vocab.__getitem__('<sos>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Process => Model Parms Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_max_sent_len(file):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        maxlen, sent_count = 0, 0\n",
    "        for l in f.readlines():\n",
    "            maxlen = max([maxlen, max([len(sent) for sent in l.split()])])\n",
    "            sent_count += 1\n",
    "    \n",
    "    return maxlen, sent_count\n",
    "\n",
    "args.max_sent_len, args.sent_count = get_max_sent_len(botTrainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1000,\n",
       " 'exts': ['.en.atok', '.de.atok'],\n",
       " 'manual_log': './manualog_transfromer1.log',\n",
       " 'max_dec_num': 50,\n",
       " 'max_enc_num': 50,\n",
       " 'max_sent_len': 242,\n",
       " 'modes': ['train', 'val', 'test2016'],\n",
       " 'n_sent': 5,\n",
       " 'ndev': 1,\n",
       " 'path': './data/multi30k/',\n",
       " 'sent_count': 454129,\n",
       " 'vocab_path': '/home/ubuntu/Studio/dialog_db/chinese_chatbot_corpus-master/clean_chat_corpus/vocab.txt'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size = 1000\n",
    "dirrm(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sentence pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ref: Seq2Seq - Transformer](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def _generate_square_subsequent_mask(sz):\n",
    "#     mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#     mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#     return mask\n",
    "\n",
    "# _generate_square_subsequent_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/geo/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 16.25 | loss  4.83 | ppl   124.74\n",
      "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 15.81 | loss  4.86 | ppl   129.21\n",
      "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 15.88 | loss  4.70 | ppl   110.18\n",
      "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 15.87 | loss  4.76 | ppl   116.74\n",
      "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 15.90 | loss  4.75 | ppl   116.15\n",
      "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 15.93 | loss  4.80 | ppl   121.55\n",
      "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 15.92 | loss  4.83 | ppl   124.85\n",
      "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 15.96 | loss  4.87 | ppl   130.16\n",
      "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 15.98 | loss  4.85 | ppl   128.21\n",
      "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 15.99 | loss  4.85 | ppl   127.33\n",
      "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 16.01 | loss  4.73 | ppl   113.63\n",
      "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 16.03 | loss  4.81 | ppl   122.16\n",
      "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 16.00 | loss  4.84 | ppl   126.46\n",
      "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 16.02 | loss  4.79 | ppl   120.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 49.99s | valid loss  5.54 | valid ppl   255.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.45 | test ppl   232.79\n",
      "=========================================================================================\n",
      "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 16.19 | loss  4.79 | ppl   120.88\n",
      "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 16.05 | loss  4.83 | ppl   125.79\n",
      "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 16.11 | loss  4.66 | ppl   105.63\n",
      "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 16.08 | loss  4.73 | ppl   112.83\n",
      "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 16.07 | loss  4.72 | ppl   112.56\n",
      "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 16.11 | loss  4.74 | ppl   114.73\n",
      "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 16.07 | loss  4.78 | ppl   119.23\n",
      "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 16.12 | loss  4.82 | ppl   124.49\n",
      "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 16.20 | loss  4.79 | ppl   120.87\n",
      "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 16.15 | loss  4.81 | ppl   122.18\n",
      "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 16.13 | loss  4.68 | ppl   107.52\n",
      "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 16.11 | loss  4.78 | ppl   118.68\n",
      "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 16.11 | loss  4.78 | ppl   119.25\n",
      "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 16.43 | loss  4.75 | ppl   115.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 50.48s | valid loss  5.54 | valid ppl   255.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.45 | test ppl   231.98\n",
      "=========================================================================================\n",
      "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 16.26 | loss  4.76 | ppl   116.55\n",
      "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 16.20 | loss  4.78 | ppl   119.37\n",
      "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 16.18 | loss  4.61 | ppl   100.90\n",
      "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 16.17 | loss  4.68 | ppl   107.84\n",
      "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 16.15 | loss  4.70 | ppl   110.09\n",
      "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 16.19 | loss  4.70 | ppl   110.04\n",
      "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 16.20 | loss  4.73 | ppl   113.21\n",
      "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 16.18 | loss  4.79 | ppl   119.86\n",
      "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 16.18 | loss  4.74 | ppl   113.96\n",
      "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 16.19 | loss  4.76 | ppl   116.30\n",
      "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 16.22 | loss  4.63 | ppl   102.90\n",
      "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 16.23 | loss  4.72 | ppl   111.81\n",
      "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 16.17 | loss  4.74 | ppl   113.89\n",
      "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 16.22 | loss  4.69 | ppl   108.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 50.66s | valid loss  5.60 | valid ppl   270.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.50 | test ppl   244.33\n",
      "=========================================================================================\n",
      "| epoch   4 |   200/ 2981 batches | lr 4.07 | ms/batch 16.34 | loss  4.71 | ppl   110.71\n",
      "| epoch   4 |   400/ 2981 batches | lr 4.07 | ms/batch 16.24 | loss  4.74 | ppl   113.88\n",
      "| epoch   4 |   600/ 2981 batches | lr 4.07 | ms/batch 16.23 | loss  4.57 | ppl    96.86\n",
      "| epoch   4 |   800/ 2981 batches | lr 4.07 | ms/batch 16.26 | loss  4.64 | ppl   103.85\n",
      "| epoch   4 |  1000/ 2981 batches | lr 4.07 | ms/batch 16.25 | loss  4.64 | ppl   103.53\n",
      "| epoch   4 |  1200/ 2981 batches | lr 4.07 | ms/batch 16.24 | loss  4.66 | ppl   105.51\n",
      "| epoch   4 |  1400/ 2981 batches | lr 4.07 | ms/batch 16.27 | loss  4.70 | ppl   109.70\n",
      "| epoch   4 |  1600/ 2981 batches | lr 4.07 | ms/batch 16.24 | loss  4.74 | ppl   113.89\n",
      "| epoch   4 |  1800/ 2981 batches | lr 4.07 | ms/batch 16.23 | loss  4.70 | ppl   110.08\n",
      "| epoch   4 |  2000/ 2981 batches | lr 4.07 | ms/batch 16.23 | loss  4.71 | ppl   110.90\n",
      "| epoch   4 |  2200/ 2981 batches | lr 4.07 | ms/batch 16.27 | loss  4.58 | ppl    97.28\n",
      "| epoch   4 |  2400/ 2981 batches | lr 4.07 | ms/batch 16.23 | loss  4.66 | ppl   106.04\n",
      "| epoch   4 |  2600/ 2981 batches | lr 4.07 | ms/batch 16.40 | loss  4.68 | ppl   108.00\n",
      "| epoch   4 |  2800/ 2981 batches | lr 4.07 | ms/batch 16.24 | loss  4.64 | ppl   104.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 50.84s | valid loss  5.47 | valid ppl   237.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.38 | test ppl   215.97\n",
      "=========================================================================================\n",
      "| epoch   5 |   200/ 2981 batches | lr 3.87 | ms/batch 16.35 | loss  4.66 | ppl   105.43\n",
      "| epoch   5 |   400/ 2981 batches | lr 3.87 | ms/batch 16.25 | loss  4.69 | ppl   108.68\n",
      "| epoch   5 |   600/ 2981 batches | lr 3.87 | ms/batch 16.23 | loss  4.52 | ppl    92.20\n",
      "| epoch   5 |   800/ 2981 batches | lr 3.87 | ms/batch 16.27 | loss  4.59 | ppl    98.83\n",
      "| epoch   5 |  1000/ 2981 batches | lr 3.87 | ms/batch 16.46 | loss  4.60 | ppl    99.38\n",
      "| epoch   5 |  1200/ 2981 batches | lr 3.87 | ms/batch 16.33 | loss  4.62 | ppl   101.23\n",
      "| epoch   5 |  1400/ 2981 batches | lr 3.87 | ms/batch 16.28 | loss  4.64 | ppl   103.23\n",
      "| epoch   5 |  1600/ 2981 batches | lr 3.87 | ms/batch 16.28 | loss  4.70 | ppl   109.45\n",
      "| epoch   5 |  1800/ 2981 batches | lr 3.87 | ms/batch 16.24 | loss  4.66 | ppl   105.55\n",
      "| epoch   5 |  2000/ 2981 batches | lr 3.87 | ms/batch 16.24 | loss  4.67 | ppl   106.78\n",
      "| epoch   5 |  2200/ 2981 batches | lr 3.87 | ms/batch 16.24 | loss  4.53 | ppl    93.19\n",
      "| epoch   5 |  2400/ 2981 batches | lr 3.87 | ms/batch 16.25 | loss  4.62 | ppl   101.20\n",
      "| epoch   5 |  2600/ 2981 batches | lr 3.87 | ms/batch 16.24 | loss  4.63 | ppl   102.31\n",
      "| epoch   5 |  2800/ 2981 batches | lr 3.87 | ms/batch 16.23 | loss  4.60 | ppl    99.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 50.88s | valid loss  5.52 | valid ppl   250.78\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.43 | test ppl   227.73\n",
      "=========================================================================================\n",
      "| epoch   6 |   200/ 2981 batches | lr 3.68 | ms/batch 16.32 | loss  4.62 | ppl   100.99\n",
      "| epoch   6 |   400/ 2981 batches | lr 3.68 | ms/batch 16.23 | loss  4.65 | ppl   104.53\n",
      "| epoch   6 |   600/ 2981 batches | lr 3.68 | ms/batch 16.22 | loss  4.48 | ppl    88.42\n",
      "| epoch   6 |   800/ 2981 batches | lr 3.68 | ms/batch 16.24 | loss  4.55 | ppl    94.48\n",
      "| epoch   6 |  1000/ 2981 batches | lr 3.68 | ms/batch 16.26 | loss  4.56 | ppl    95.80\n",
      "| epoch   6 |  1200/ 2981 batches | lr 3.68 | ms/batch 16.25 | loss  4.59 | ppl    98.20\n",
      "| epoch   6 |  1400/ 2981 batches | lr 3.68 | ms/batch 16.24 | loss  4.59 | ppl    98.82\n",
      "| epoch   6 |  1600/ 2981 batches | lr 3.68 | ms/batch 16.26 | loss  4.65 | ppl   104.09\n",
      "| epoch   6 |  1800/ 2981 batches | lr 3.68 | ms/batch 16.24 | loss  4.61 | ppl   100.80\n",
      "| epoch   6 |  2000/ 2981 batches | lr 3.68 | ms/batch 16.25 | loss  4.62 | ppl   101.23\n",
      "| epoch   6 |  2200/ 2981 batches | lr 3.68 | ms/batch 16.23 | loss  4.49 | ppl    89.15\n",
      "| epoch   6 |  2400/ 2981 batches | lr 3.68 | ms/batch 16.25 | loss  4.56 | ppl    95.79\n",
      "| epoch   6 |  2600/ 2981 batches | lr 3.68 | ms/batch 16.28 | loss  4.59 | ppl    98.30\n",
      "| epoch   6 |  2800/ 2981 batches | lr 3.68 | ms/batch 16.27 | loss  4.55 | ppl    94.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 50.85s | valid loss  5.58 | valid ppl   265.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.49 | test ppl   241.62\n",
      "=========================================================================================\n",
      "| epoch   7 |   200/ 2981 batches | lr 3.49 | ms/batch 16.34 | loss  4.57 | ppl    96.52\n",
      "| epoch   7 |   400/ 2981 batches | lr 3.49 | ms/batch 16.22 | loss  4.60 | ppl    99.56\n",
      "| epoch   7 |   600/ 2981 batches | lr 3.49 | ms/batch 16.24 | loss  4.44 | ppl    85.02\n",
      "| epoch   7 |   800/ 2981 batches | lr 3.49 | ms/batch 16.31 | loss  4.51 | ppl    90.92\n",
      "| epoch   7 |  1000/ 2981 batches | lr 3.49 | ms/batch 16.23 | loss  4.51 | ppl    90.94\n",
      "| epoch   7 |  1200/ 2981 batches | lr 3.49 | ms/batch 16.24 | loss  4.54 | ppl    93.24\n",
      "| epoch   7 |  1400/ 2981 batches | lr 3.49 | ms/batch 16.29 | loss  4.55 | ppl    94.85\n",
      "| epoch   7 |  1600/ 2981 batches | lr 3.49 | ms/batch 16.34 | loss  4.60 | ppl    99.94\n",
      "| epoch   7 |  1800/ 2981 batches | lr 3.49 | ms/batch 16.25 | loss  4.56 | ppl    95.94\n",
      "| epoch   7 |  2000/ 2981 batches | lr 3.49 | ms/batch 16.24 | loss  4.60 | ppl    99.50\n",
      "| epoch   7 |  2200/ 2981 batches | lr 3.49 | ms/batch 16.23 | loss  4.46 | ppl    86.11\n",
      "| epoch   7 |  2400/ 2981 batches | lr 3.49 | ms/batch 16.22 | loss  4.53 | ppl    92.70\n",
      "| epoch   7 |  2600/ 2981 batches | lr 3.49 | ms/batch 16.23 | loss  4.54 | ppl    93.55\n",
      "| epoch   7 |  2800/ 2981 batches | lr 3.49 | ms/batch 16.27 | loss  4.52 | ppl    91.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 50.84s | valid loss  5.53 | valid ppl   252.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.43 | test ppl   228.02\n",
      "=========================================================================================\n",
      "| epoch   8 |   200/ 2981 batches | lr 3.32 | ms/batch 16.39 | loss  4.54 | ppl    93.54\n",
      "| epoch   8 |   400/ 2981 batches | lr 3.32 | ms/batch 16.70 | loss  4.56 | ppl    95.67\n",
      "| epoch   8 |   600/ 2981 batches | lr 3.32 | ms/batch 16.30 | loss  4.40 | ppl    81.14\n",
      "| epoch   8 |   800/ 2981 batches | lr 3.32 | ms/batch 16.27 | loss  4.47 | ppl    87.74\n",
      "| epoch   8 |  1000/ 2981 batches | lr 3.32 | ms/batch 16.23 | loss  4.48 | ppl    87.80\n",
      "| epoch   8 |  1200/ 2981 batches | lr 3.32 | ms/batch 16.29 | loss  4.49 | ppl    88.87\n",
      "| epoch   8 |  1400/ 2981 batches | lr 3.32 | ms/batch 16.26 | loss  4.51 | ppl    91.28\n",
      "| epoch   8 |  1600/ 2981 batches | lr 3.32 | ms/batch 16.27 | loss  4.57 | ppl    96.35\n",
      "| epoch   8 |  1800/ 2981 batches | lr 3.32 | ms/batch 16.26 | loss  4.54 | ppl    93.68\n",
      "| epoch   8 |  2000/ 2981 batches | lr 3.32 | ms/batch 16.28 | loss  4.55 | ppl    94.20\n",
      "| epoch   8 |  2200/ 2981 batches | lr 3.32 | ms/batch 16.27 | loss  4.41 | ppl    82.05\n",
      "| epoch   8 |  2400/ 2981 batches | lr 3.32 | ms/batch 16.29 | loss  4.49 | ppl    88.92\n",
      "| epoch   8 |  2600/ 2981 batches | lr 3.32 | ms/batch 16.31 | loss  4.51 | ppl    90.56\n",
      "| epoch   8 |  2800/ 2981 batches | lr 3.32 | ms/batch 16.23 | loss  4.48 | ppl    88.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 51.00s | valid loss  5.56 | valid ppl   260.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.48 | test ppl   238.70\n",
      "=========================================================================================\n",
      "| epoch   9 |   200/ 2981 batches | lr 3.15 | ms/batch 16.34 | loss  4.50 | ppl    89.84\n",
      "| epoch   9 |   400/ 2981 batches | lr 3.15 | ms/batch 16.27 | loss  4.52 | ppl    92.25\n",
      "| epoch   9 |   600/ 2981 batches | lr 3.15 | ms/batch 16.26 | loss  4.37 | ppl    78.85\n",
      "| epoch   9 |   800/ 2981 batches | lr 3.15 | ms/batch 16.30 | loss  4.43 | ppl    84.13\n",
      "| epoch   9 |  1000/ 2981 batches | lr 3.15 | ms/batch 16.27 | loss  4.44 | ppl    84.96\n",
      "| epoch   9 |  1200/ 2981 batches | lr 3.15 | ms/batch 16.26 | loss  4.46 | ppl    86.33\n",
      "| epoch   9 |  1400/ 2981 batches | lr 3.15 | ms/batch 16.26 | loss  4.47 | ppl    87.59\n",
      "| epoch   9 |  1600/ 2981 batches | lr 3.15 | ms/batch 16.25 | loss  4.53 | ppl    92.95\n",
      "| epoch   9 |  1800/ 2981 batches | lr 3.15 | ms/batch 16.23 | loss  4.50 | ppl    90.27\n",
      "| epoch   9 |  2000/ 2981 batches | lr 3.15 | ms/batch 16.29 | loss  4.51 | ppl    91.25\n",
      "| epoch   9 |  2200/ 2981 batches | lr 3.15 | ms/batch 16.27 | loss  4.39 | ppl    80.37\n",
      "| epoch   9 |  2400/ 2981 batches | lr 3.15 | ms/batch 16.32 | loss  4.45 | ppl    85.73\n",
      "| epoch   9 |  2600/ 2981 batches | lr 3.15 | ms/batch 16.31 | loss  4.48 | ppl    87.89\n",
      "| epoch   9 |  2800/ 2981 batches | lr 3.15 | ms/batch 16.24 | loss  4.44 | ppl    84.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 50.89s | valid loss  5.52 | valid ppl   250.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.43 | test ppl   228.59\n",
      "=========================================================================================\n",
      "| epoch  10 |   200/ 2981 batches | lr 2.99 | ms/batch 16.36 | loss  4.47 | ppl    87.14\n",
      "| epoch  10 |   400/ 2981 batches | lr 2.99 | ms/batch 16.29 | loss  4.49 | ppl    89.18\n",
      "| epoch  10 |   600/ 2981 batches | lr 2.99 | ms/batch 16.26 | loss  4.33 | ppl    76.15\n",
      "| epoch  10 |   800/ 2981 batches | lr 2.99 | ms/batch 16.24 | loss  4.40 | ppl    81.28\n",
      "| epoch  10 |  1000/ 2981 batches | lr 2.99 | ms/batch 16.26 | loss  4.41 | ppl    82.12\n",
      "| epoch  10 |  1200/ 2981 batches | lr 2.99 | ms/batch 16.25 | loss  4.43 | ppl    83.61\n",
      "| epoch  10 |  1400/ 2981 batches | lr 2.99 | ms/batch 16.31 | loss  4.44 | ppl    84.76\n",
      "| epoch  10 |  1600/ 2981 batches | lr 2.99 | ms/batch 16.29 | loss  4.49 | ppl    89.57\n",
      "| epoch  10 |  1800/ 2981 batches | lr 2.99 | ms/batch 16.28 | loss  4.46 | ppl    86.51\n",
      "| epoch  10 |  2000/ 2981 batches | lr 2.99 | ms/batch 16.28 | loss  4.47 | ppl    87.74\n",
      "| epoch  10 |  2200/ 2981 batches | lr 2.99 | ms/batch 16.28 | loss  4.34 | ppl    76.77\n",
      "| epoch  10 |  2400/ 2981 batches | lr 2.99 | ms/batch 16.29 | loss  4.42 | ppl    83.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |  2600/ 2981 batches | lr 2.99 | ms/batch 16.26 | loss  4.44 | ppl    85.00\n",
      "| epoch  10 |  2800/ 2981 batches | lr 2.99 | ms/batch 16.24 | loss  4.41 | ppl    82.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 50.88s | valid loss  5.59 | valid ppl   268.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.51 | test ppl   246.87\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "bptt = 35\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10  # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "\n",
    "if os.path.isfile(args.model_path):\n",
    "    model.load_state_dict(torch.load(args.model_path))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), f = args.model_path)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    test_loss = evaluate(best_model, test_data)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T,R,K; Run initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "\n",
    "predict_model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "predict_model.load_state_dict(torch.load(args.model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results  \n",
    "bptt = 35\n",
    "i = bptt * 2\n",
    "# data, targets = get_batch(test_data, i)\n",
    "data, targets = get_batch(train_data, i)\n",
    "# best_model.eval()\n",
    "output = predict_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data2sent(data, func = None):\n",
    "    if func:\n",
    "        return [[TEXT.vocab.itos[func(ind).data.item()] for ind in data[:, i]]\n",
    "            for i in range(data.shape[1])]\n",
    "    \n",
    "    else:\n",
    "        return [[TEXT.vocab.itos[ind.data.item()] for ind in data[:, i]]\n",
    "            for i in range(data.shape[1])]\n",
    "    \n",
    "    \n",
    "# data2sent(data)\n",
    "data2sent(predict_model(data[1:10,:]), func = lambda word_tensor: torch.argmax(word_tensor, dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
